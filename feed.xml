<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="https://samyclem.github.io/blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://samyclem.github.io/blog/" rel="alternate" type="text/html" /><updated>2020-02-18T09:04:54-06:00</updated><id>https://samyclem.github.io/blog/feed.xml</id><title type="html">Samy Clementz</title><entry><title type="html">A brief introduction to density estimation</title><link href="https://samyclem.github.io/blog/2020/02/15/density-estimation.html" rel="alternate" type="text/html" title="A brief introduction to density estimation" /><published>2020-02-15T00:00:00-06:00</published><updated>2020-02-15T00:00:00-06:00</updated><id>https://samyclem.github.io/blog/2020/02/15/density-estimation</id><content type="html" xml:base="https://samyclem.github.io/blog/2020/02/15/density-estimation.html">&lt;p&gt;Let $(X_1,\ldots,X_n)$ be identically distributed real valued random variables whose common distribution has density $p_X$ with respect to the Lebesgue measure, i.e. $\mathbb{P}(X \in A) = \int_{A}p_X(x)dx$ for all $A \subset \mathbb{R}$. Our goal is to estimate $p_X$ from the data $(X_1,\ldots,X_n)$.&lt;/p&gt;

&lt;h3 id=&quot;the-parametric-way&quot;&gt;The parametric way&lt;/h3&gt;

&lt;p&gt;One classic way to do so is to assume that $p_X$ belongs to a parametric family &lt;script type=&quot;math/tex&quot;&gt;\{f_{\theta} : \theta \in \Theta \}&lt;/script&gt; where $\Theta$ is a subset of $\mathbb{R}^k$ with a fixed dimension $k$. For example, one may assume that the data $(X_1,\ldots,X_n)$ was sampled from a normal distribution. In this case, $k=2$, &lt;script type=&quot;math/tex&quot;&gt;\Theta = \{(\mu, \sigma^2) : \mu \in \mathbb{R}, \sigma &gt; 0\}&lt;/script&gt; and $f_{\theta}(.) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(. - \mu)^2}{2\sigma^2}}$.&lt;/p&gt;

&lt;p&gt;How do we estimate the parameter $\theta$ ? A theoretically-justified method of estimation is to choose $\hat{\theta}$ that maximizes the log-likelihood $l_\theta$ of the data :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;l(\theta) = \sum_{i=1}^n \log f_{\theta}(X_i)&lt;/script&gt;

&lt;p&gt;Now, we hope that computing the solutions of $\nabla l(\theta) = 0$ gives one and only one maximum likelihood estimate (MLE). This is the case for the above example : the MLE is $\hat{\theta} = (\hat{\mu}, \hat{\sigma}^2)$ where $\hat{\mu} = \frac{1}{n}\sum_{i=1}^nX_i$ and $\hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^n(X_i - \hat{\mu})^2$, as expected.&lt;/p&gt;

&lt;h3 id=&quot;the-nonparametric-way&quot;&gt;The nonparametric way&lt;/h3&gt;

&lt;p&gt;Unfortunately, the data we gather can be too complex and prior information about $p_X$ may be not available. Murray Rosenblatt, in an article published in 1956, suggested a nonparametric method for estimating $p_X$ using the weak assumption that $p_X$ is continuous function. Here’s his reasoning :&lt;/p&gt;

&lt;p&gt;If $p_X$ is continuous, then the cumulative distribution function of $X$ defined by $F_X(x) = \mathbb{P}(X\leq x)$ for all $x \in \mathbb{R}$ is differentiable and its derivative is continuous. Indeed, $F_X(x) = \int_{-\infty}^{x}p_X(u)du$, so $F_X’(x) = p_X(x)$ for all $x \in \mathbb{R}$, which means that for $h &amp;gt; 0$ small :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p_X(x) \approx \frac{F_X(x+h) - F_X(x-h)}{2h}.&lt;/script&gt;

&lt;p&gt;As an estimate of $F_n$, let’s use the usual empirical cumulative distribution function defined as &lt;script type=&quot;math/tex&quot;&gt;\hat{F}_n(x) = \frac{1}{n}\sum_{i=1}^n 1_{X_i \leq x}&lt;/script&gt;. Then, our estimate of $p_X$ is :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{p}_h(x) = \frac{\hat{F}_n(x+h) - \hat{F}_n(x-h)}{2h},&lt;/script&gt;

&lt;p&gt;Where $h&amp;gt;0$ is the bandwidth hyperparameter.&lt;/p&gt;

&lt;p&gt;Let’s write it differently so we can better understand what’s going on.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\hat{p}_h(x) = \frac{1}{2nh}\sum_{i=1}^n 1_{x-h&lt;X_i\leq x+h}. %]]&gt;&lt;/script&gt;

&lt;p&gt;Everything is clear now. What we do to estimate $p_X(x)$ is just counting the training examples $X_i$ which are lying within distance $h$ from $x$ and finally dividing this count by the good constant so that $\hat{p}_h$ is a probability density function.&lt;/p&gt;

&lt;p&gt;[insert graph bandwidth]&lt;/p&gt;

&lt;p&gt;Three problems :&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We get a discontinuous density estimate.&lt;/li&gt;
&lt;li&gt;The estimator gives the same attention to the data very close to $x$ than those at distance a bit less than h. We also discard all the data further.&lt;/li&gt;
&lt;li&gt;How do we choose the hyperparameter $h$ ?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I will now define what a kernel is, and then I will explain why kernels solve both of these problems.&lt;/p&gt;

&lt;p&gt;A kernel is a positive function $K : \mathbb{R} \rightarrow \mathbb{R}$ such that :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\int_{-\infty}^{\infty}K(u)du = 1&lt;/script&gt;

&lt;p&gt;The kernels we will be using will play a role similar as those we encounter in Fourier analysis. Because of this, we don’t call them probability density functions, even though they share essentially the same definition.&lt;/p&gt;

&lt;p&gt;The kernels we will be using will play a role similar as those we encounter in real analysis. Because of this, we don’t call them probability density functions, even though they share essentially the same definition.&lt;/p&gt;

&lt;p&gt;We can rewrite our estimator in the form :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{p}_h(x) = \frac{1}{nh}\sum_{i=1}^n K_{unit}(\frac{X_i-x}{h}),&lt;/script&gt;

&lt;p&gt;where $K_{unit}(u) = \frac{1}{2}1_{1&amp;lt;u\leq 1}$ is the unit kernel.&lt;/p&gt;

&lt;p&gt;We can now replace $K_{unit}$ by a smooth kernel that gives less and less importance to the training samples furthest from $x$.&lt;/p&gt;

&lt;p&gt;[give examples of kernels]&lt;/p&gt;

&lt;p&gt;[graph inference]&lt;/p&gt;

&lt;p&gt;\subsection{Mean squared error of kernel estimators}&lt;/p&gt;

&lt;p&gt;Now fix $x \in \mathbb{R}$. We want to assess the performance of $\hat{p}_h(x)$ as an estimate of $p(x)$. This is a problem of point estimation. A common way to measure the accuracy of an point estimator is to look at its mean squared risk :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{MSE}_p(x) = \mathbb{E}_{p}[(\hat{p}_h(x) - p(x))^2].&lt;/script&gt;

&lt;p&gt;Here the expectation is taken with respect to all the possible traning examples $(X_1,\ldots,X_n) \sim p$. As usual, the MSE can be decomposed as the sum of the squared bias and the variance of $\hat{p}_h(x)$ :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;MSE_p(x) = b_p(x)^2 + \text{Var}_p(\hat{p}_h(x)),&lt;/script&gt;

&lt;p&gt;where $b_p(x) = \mathbb{E}_p(\hat{p}_h(x)) - p(x)$ and $\text{Var}_p(\hat{p}_h(x)) = \mathbb{E}_p[\hat{p}_h(x)^2] - \mathbb{E}_p[\hat{p}_h(x)]^2$.&lt;/p&gt;

&lt;p&gt;We will now give upper bounds on these two terms.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Upper bound on the variance&lt;/strong&gt;: $\hat{p}_h(x)$ is a sum of functions of i.i.d. random variables, so:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{Var}_p(\hat{p}_h(x)) = \frac{1}{nh^2}\text{Var}_p[K(\frac{X_1 - x}{h})].&lt;/script&gt;

&lt;p&gt;König-Huygens formula gives us an upper bound for this variance :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{Var}_p(\hat{p}_h(x)) \leq \frac{1}{nh^2}\mathbb{E}_p[K^2(\frac{X_1 - x}{h})].&lt;/script&gt;

&lt;p&gt;But $\mathbb{E}&lt;em&gt;p[K^2(\frac{X_1 - x}{h})] = \int&lt;/em&gt;{-\infty}^{+\infty}K^2(\frac{u - x}{h})p(u)du = h\int_{-\infty}^{+\infty}K^2(u)p(x+uh)du.$&lt;/p&gt;

&lt;p&gt;Finally, we see that :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{Var}_p(\hat{p}_h(x)) \leq \frac{C_1}{nh},&lt;/script&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;where $C_1 =&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;p&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;&lt;em&gt;\infty\int&lt;/em&gt;{-\infty}^{+\infty}K^2(u)du$.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;We see that when $h$ is large, then we have small variance. This is no surprise because then a lot of training examples will be taken into account when making inference about $p(x)$, so having a large bandwidth $h$ adds stability.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Upper bound on the bias:&lt;/strong&gt; Using the property $\int_{-\infty}^{+\infty}K(u)du = 1$ we get:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;b_p(x) = \int_{-\infty}^{+\infty}K(u)[p(x + uh) - p(x)]du.&lt;/script&gt;

&lt;p&gt;This expression of $b_p(x)$ allows us to see the role of the regularity of $p$. It is natural to think that the most regular $p$ is, the better our estimate will be,  because when making inference about $p(x)$ we only have access to the nearest training example $(X_1,\ldots,X_n)$ which were generated using the amplitude of $p$ near them, so we do not want $p$ to be too wiggly around $x$.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Let’s assume that $p$ is $L$-Lipschitz, where $L$ is a positive real number. This means that $&lt;/td&gt;
      &lt;td&gt;p(u) - p(v)&lt;/td&gt;
      &lt;td&gt;\leq L&lt;/td&gt;
      &lt;td&gt;u-v&lt;/td&gt;
      &lt;td&gt;$ for all $(u,v) \in \mathbb{R}^2$. Then:&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;|b_p(x)| \leq Lh\int_{-\infty}^{+\infty} K(u)|u|du,&lt;/script&gt;

&lt;p&gt;so:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;b_p(x)^2 \leq C_2h^2,&lt;/script&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;where $C_2 = L\int_{-\infty}^{+\infty}K(u)&lt;/td&gt;
      &lt;td&gt;u&lt;/td&gt;
      &lt;td&gt;du$.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Finally we get that the mean squared error is upper bounded by the sum of two terms :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{MSE}_p(x) \leq \frac{C_1}{nh} + C_2h^2.&lt;/script&gt;

&lt;p&gt;\subsection{The role of h and the Bias-variance tradeoff}&lt;/p&gt;</content><author><name></name></author><summary type="html">Let $(X_1,\ldots,X_n)$ be identically distributed real valued random variables whose common distribution has density $p_X$ with respect to the Lebesgue measure, i.e. $\mathbb{P}(X \in A) = \int_{A}p_X(x)dx$ for all $A \subset \mathbb{R}$. Our goal is to estimate $p_X$ from the data $(X_1,\ldots,X_n)$.</summary></entry></feed>