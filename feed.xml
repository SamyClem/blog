<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="https://samyclem.github.io/blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://samyclem.github.io/blog/" rel="alternate" type="text/html" /><updated>2020-02-18T03:57:46-06:00</updated><id>https://samyclem.github.io/blog/feed.xml</id><title type="html">Samy Clementz</title><entry><title type="html">A brief introduction to density estimation</title><link href="https://samyclem.github.io/blog/2020/02/15/density-estimation.html" rel="alternate" type="text/html" title="A brief introduction to density estimation" /><published>2020-02-15T00:00:00-06:00</published><updated>2020-02-15T00:00:00-06:00</updated><id>https://samyclem.github.io/blog/2020/02/15/density-estimation</id><content type="html" xml:base="https://samyclem.github.io/blog/2020/02/15/density-estimation.html">&lt;p&gt;Let $(X_1,\ldots,X_n)$ be identically distributed real valued random variables whose common distribution has density $p_X$ with respect to the Lebesgue measure, i.e. $\mathbb{P}(X \in A) = \int_{A}p_X(x)dx$ for all $A \subset \mathbb{R}$. Our goal is to estimate $p_X$ from the data $(X_1,\ldots,X_n)$.&lt;/p&gt;

&lt;h3 id=&quot;the-parametric-way&quot;&gt;The parametric way&lt;/h3&gt;

&lt;p&gt;One classic way to do so is to assume that $p_X$ belongs to a parametric family &lt;script type=&quot;math/tex&quot;&gt;\{f_{\theta} : \theta \in \Theta \}&lt;/script&gt; where $\Theta$ is a subset of $\mathbb{R}^k$ with a fixed dimension $k$. For example, one may assume that the data $(X_1,\ldots,X_n)$ was sampled from a normal distribution. In this case, $k=2$, &lt;script type=&quot;math/tex&quot;&gt;\Theta = \{(\mu, \sigma^2) : \mu \in \mathbb{R}, \sigma &gt; 0\}&lt;/script&gt; and $f_{\theta}(.) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(. - \mu)^2}{2\sigma^2}}$.&lt;/p&gt;

&lt;p&gt;How do we estimate the parameter $\theta$ ? A theoretically-justified method of estimation is to choose $\hat{\theta}$ that maximizes the log-likelihood $l_\theta$ of the data :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;l(\theta) = \sum_{i=1}^n \log f_{\theta}(X_i)&lt;/script&gt;

&lt;p&gt;Now, we hope that computing the solutions of $\nabla l(\theta) = 0$ gives one and only one maximum likelihood estimate (MLE). This is the case for the above example : the MLE is $\hat{\theta} = (\hat{\mu}, \hat{\sigma}^2)$ where $\hat{\mu} = \frac{1}{n}\sum_{i=1}^nX_i$ and $\hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^n(X_i - \hat{\mu})^2$, as expected.&lt;/p&gt;

&lt;h3 id=&quot;the-nonparametric-way&quot;&gt;The nonparametric way&lt;/h3&gt;

&lt;p&gt;Unfortunately, the data we gather can be too complex and prior information about $p_X$ may be not available. Murray Rosenblatt, in an article published in 1956, suggested a nonparametric method for estimating $p_X$ using the weak assumption that $p_X$ is continuous function. Here’s his reasoning :&lt;/p&gt;

&lt;p&gt;If $p_X$ is continuous, then the cumulative distribution function of $X$ defined by $F_X(x) = \mathbb{P}(X\leq x)$ for all $x \in \mathbb{R}$ is differentiable and its derivative is continuous. Indeed, $F_X(x) = \int_{-\infty}^{x}p_X(u)du$, so $F_X’(x) = p_X(x)$ for all $x \in \mathbb{R}$, which means that for $h &amp;gt; 0$ small :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p_X(x) \approx \frac{F_X(x+h) - F_X(x-h)}{2h}.&lt;/script&gt;

&lt;p&gt;As an estimate of $F_n$, let’s use the usual empirical cumulative distribution function defined as &lt;script type=&quot;math/tex&quot;&gt;\hat{F}_n(x) = \frac{1}{n}\sum_{i=1}^n 1_{X_i \leq x}&lt;/script&gt;. Then, our estimate of $p_X$ is :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{p}_h(x) = \frac{\hat{F}_n(x+h) - \hat{F}_n(x-h)}{2h},&lt;/script&gt;

&lt;p&gt;Where $h&amp;gt;0$ is the bandwidth hyperparameter.&lt;/p&gt;

&lt;p&gt;Let’s write it differently so we can better understand what’s going on.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\hat{p}_h(x) = \frac{1}{2nh}\sum_{i=1}^n 1_{x-h&lt;X_i\leq x+h} %]]&gt;&lt;/script&gt;

&lt;p&gt;Everything is clear now. What we do to estimate $p_X(x)$ is just counting the training examples $X_i$ which are lying within distance $h$ from $x$ and finally dividing this count by the good constant so that $\hat{p}_h$ is a probability density function.&lt;/p&gt;

&lt;p&gt;[insert graph bandwidth]&lt;/p&gt;

&lt;p&gt;Two problems: 1) discontinuous estimate and 2) give the same attention to the data very close than those at distance a bit less than h. We also discard all the data further.&lt;/p&gt;

&lt;p&gt;I will now define what a kernel is, and then I will explain why kernels solve both of these problems.&lt;/p&gt;

&lt;p&gt;A kernel is a positive function $K : \mathbb{R} \rightarrow \mathbb{R}$ such that :&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\int_{-\infty}^{\infty}K(u)du = 1&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;The kernels we will be using will play a role similar as those we encounter in Fourier analysis. Because of this, we don’t call them probability density functions, even though they share essentially the same definition.&lt;/p&gt;</content><author><name></name></author><summary type="html">Let $(X_1,\ldots,X_n)$ be identically distributed real valued random variables whose common distribution has density $p_X$ with respect to the Lebesgue measure, i.e. $\mathbb{P}(X \in A) = \int_{A}p_X(x)dx$ for all $A \subset \mathbb{R}$. Our goal is to estimate $p_X$ from the data $(X_1,\ldots,X_n)$.</summary></entry></feed>