<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><link rel="shortcut icon" type="image/x-icon" href="/blog/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>A brief introduction to density estimation | Samy Clementz</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="A brief introduction to density estimation" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Let $(X_1,\ldots,X_n)$ be identically distributed real valued random variables whose common distribution has density $p_X$ with respect to the Lebesgue measure, i.e. $\mathbb{P}(X \in A) = \int_{A}p_X(x)dx$ for all $A \subset \mathbb{R}$. Our goal is to estimate $p_X$ from the data $(X_1,\ldots,X_n)$." />
<meta property="og:description" content="Let $(X_1,\ldots,X_n)$ be identically distributed real valued random variables whose common distribution has density $p_X$ with respect to the Lebesgue measure, i.e. $\mathbb{P}(X \in A) = \int_{A}p_X(x)dx$ for all $A \subset \mathbb{R}$. Our goal is to estimate $p_X$ from the data $(X_1,\ldots,X_n)$." />
<link rel="canonical" href="https://samyclem.github.io/blog/2020/02/15/density-estimation.html" />
<meta property="og:url" content="https://samyclem.github.io/blog/2020/02/15/density-estimation.html" />
<meta property="og:site_name" content="Samy Clementz" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-02-15T00:00:00-06:00" />
<script type="application/ld+json">
{"description":"Let $(X_1,\\ldots,X_n)$ be identically distributed real valued random variables whose common distribution has density $p_X$ with respect to the Lebesgue measure, i.e. $\\mathbb{P}(X \\in A) = \\int_{A}p_X(x)dx$ for all $A \\subset \\mathbb{R}$. Our goal is to estimate $p_X$ from the data $(X_1,\\ldots,X_n)$.","@type":"BlogPosting","url":"https://samyclem.github.io/blog/2020/02/15/density-estimation.html","dateModified":"2020-02-15T00:00:00-06:00","datePublished":"2020-02-15T00:00:00-06:00","headline":"A brief introduction to density estimation","mainEntityOfPage":{"@type":"WebPage","@id":"https://samyclem.github.io/blog/2020/02/15/density-estimation.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  <link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
  <link rel="stylesheet" href="/blog/assets/main.css"><link type="application/atom+xml" rel="alternate" href="https://samyclem.github.io/blog/feed.xml" title="Samy Clementz" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
      document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
          delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
          ]}
        );
      });
    </script>
  

  <script>
  function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
  }
  window.onload = wrap_img;
  </script>

</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">Samy Clementz</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">A brief introduction to density estimation</h1>
    <p class="post-meta"><time class="dt-published" datetime="2020-02-15T00:00:00-06:00" itemprop="datePublished">
        Feb 15, 2020
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>Let $(X_1,\ldots,X_n)$ be identically distributed real valued random variables whose common distribution has density $p_X$ with respect to the Lebesgue measure, i.e. $\mathbb{P}(X \in A) = \int_{A}p_X(x)dx$ for all $A \subset \mathbb{R}$. Our goal is to estimate $p_X$ from the data $(X_1,\ldots,X_n)$.</p>

<h3 id="the-parametric-way">The parametric way</h3>

<p>One classic way to do so is to assume that $p_X$ belongs to a parametric family <script type="math/tex">\{f_{\theta} : \theta \in \Theta \}</script> where $\Theta$ is a subset of $\mathbb{R}^k$ with a fixed dimension $k$. For example, one may assume that the data $(X_1,\ldots,X_n)$ was sampled from a normal distribution. In this case, $k=2$, <script type="math/tex">\Theta = \{(\mu, \sigma^2) : \mu \in \mathbb{R}, \sigma > 0\}</script> and $f_{\theta}(.) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(. - \mu)^2}{2\sigma^2}}$.</p>

<p>How do we estimate the parameter $\theta$ ? A theoretically-justified method of estimation is to choose $\hat{\theta}$ that maximizes the log-likelihood $l_\theta$ of the data :</p>

<script type="math/tex; mode=display">l(\theta) = \sum_{i=1}^n \log f_{\theta}(X_i)</script>

<p>Now, we hope that computing the solutions of $\nabla l(\theta) = 0$ gives one and only one maximum likelihood estimate (MLE). This is the case for the above example : the MLE is $\hat{\theta} = (\hat{\mu}, \hat{\sigma}^2)$ where $\hat{\mu} = \frac{1}{n}\sum_{i=1}^nX_i$ and $\hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^n(X_i - \hat{\mu})^2$, as expected.</p>

<h3 id="the-nonparametric-way">The nonparametric way</h3>

<p>Unfortunately, the data we gather can be too complex and prior information about $p_X$ may be not available. Murray Rosenblatt, in an article published in 1956, suggested a nonparametric method for estimating $p_X$ using the weak assumption that $p_X$ is continuous function. Here’s his reasoning :</p>

<p>If $p_X$ is continuous, then the cumulative distribution function of $X$ defined by $F_X(x) = \mathbb{P}(X\leq x)$ for all $x \in \mathbb{R}$ is differentiable and its derivative is continuous. Indeed, $F_X(x) = \int_{-\infty}^{x}p_X(u)du$, so $F_X’(x) = p_X(x)$ for all $x \in \mathbb{R}$, which means that for $h &gt; 0$ small :</p>

<script type="math/tex; mode=display">p_X(x) \approx \frac{F_X(x+h) - F_X(x-h)}{2h}.</script>

<p>As an estimate of $F_n$, let’s use the usual empirical cumulative distribution function defined as <script type="math/tex">\hat{F}_n(x) = \frac{1}{n}\sum_{i=1}^n 1_{X_i \leq x}</script>. Then, our estimate of $p_X$ is :</p>

<script type="math/tex; mode=display">\hat{p}_h(x) = \frac{\hat{F}_n(x+h) - \hat{F}_n(x-h)}{2h},</script>

<p>Where $h&gt;0$ is the bandwidth hyperparameter.</p>

<p>Let’s write it differently so we can better understand what’s going on.</p>

<script type="math/tex; mode=display">% <![CDATA[
\hat{p}_h(x) = \frac{1}{2nh}\sum_{i=1}^n 1_{x-h<X_i\leq x+h}. %]]></script>

<p>Everything is clear now. What we do to estimate $p_X(x)$ is just counting the training examples $X_i$ which are lying within distance $h$ from $x$ and finally dividing this count by the good constant so that $\hat{p}_h$ is a probability density function.</p>

<p>[insert graph bandwidth]</p>

<p>Three problems :</p>
<ol>
<li>We get a discontinuous density estimate.</li>
<li>The estimator gives the same attention to the data very close to $x$ than those at distance a bit less than h. We also discard all the data further.</li>
<li>How do we choose the hyperparameter $h$ ?</li>
</ol>

<p>I will now define what a kernel is, and then I will explain why kernels solve both of these problems.</p>

<p>A kernel is a positive function $K : \mathbb{R} \rightarrow \mathbb{R}$ such that :</p>

<script type="math/tex; mode=display">\int_{-\infty}^{\infty}K(u)du = 1</script>

<p>The kernels we will be using will play a role similar as those we encounter in Fourier analysis. Because of this, we don’t call them probability density functions, even though they share essentially the same definition.</p>

<p>The kernels we will be using will play a role similar as those we encounter in real analysis. Because of this, we don’t call them probability density functions, even though they share essentially the same definition.</p>

<p>We can rewrite our estimator in the form :</p>

<script type="math/tex; mode=display">\hat{p}_h(x) = \frac{1}{nh}\sum_{i=1}^n K_{unit}(\frac{X_i-x}{h}),</script>

<p>where $K_{unit}(u) = \frac{1}{2}1_{1&lt;u\leq 1}$ is the unit kernel.</p>

<p>We can now replace $K_{unit}$ by a smooth kernel that gives less and less importance to the training samples furthest from $x$.</p>

<p>Now fix $x \in \mathbb{R}$. We want to assess the performance of $\hat{p}_h(x)$ as an estimate of $p(x)$. This is a problem of point estimation. A common way to measure the accuracy of an point estimator is to look at its mean squared risk :</p>

<script type="math/tex; mode=display">\text{MSE}_p(x) = \mathbb{E}_{p}[(\hat{p}_h(x) - p(x))^2].</script>

<p>Here the expectation is taken with respect to all the possible traning examples $(X_1,\ldots,X_n) \sim p$. As usual, the MSE can be decomposed as the sum of the squared bias and the variance of $\hat{p}_h(x)$ :</p>

<script type="math/tex; mode=display">MSE_p(x) = b_p(x)^2 + \text{Var}_p(\hat{p}_h(x)),</script>

<p>where $b_p(x) = \mathbb{E}_p(\hat{p}_h(x)) - p(x)$ and $\text{Var}_p(\hat{p}_h(x)) = \mathbb{E}_p[\hat{p}_h(x)^2] - \mathbb{E}_p[\hat{p}_h(x)]^2$.</p>

<p>We will now give upper bounds on these two terms.</p>

<p><strong>Upper bound on the variance</strong>: $\hat{p}_h(x)$ is a sum of functions of i.i.d. random variables, so:</p>

<script type="math/tex; mode=display">\text{Var}_p(\hat{p}_h(x)) = \frac{1}{nh^2}\text{Var}_p[K(\frac{X_1 - x}{h})].</script>

<p>König-Huygens formula gives us an upper bound for this variance :</p>

<script type="math/tex; mode=display">\text{Var}_p(\hat{p}_h(x)) \leq \frac{1}{nh^2}\mathbb{E}_p[K^2(\frac{X_1 - x}{h})].</script>

<p>But :</p>

<script type="math/tex; mode=display">\mathbb{E}_p[K^2(\frac{X_1 - x}{h})] = \int_{-\infty}^{+\infty}K^2(\frac{u - x}{h})p(u)du = h\int_{-\infty}^{+\infty}K^2(u)p(x+uh)du.</script>

<p>Finally, we see that :</p>

<script type="math/tex; mode=display">\text{Var}_p(\hat{p}_h(x)) \leq \frac{C_1}{nh},</script>

<p>where <script type="math/tex">C_1 = \|p\|_\infty\int_{-\infty}^{+\infty}K^2(u)du</script>.</p>

<p>We see that when $h$ is large, then we have small variance. This is no surprise because then a lot of training examples will be taken into account when making inference about $p(x)$, so having a large bandwidth $h$ adds stability.</p>

<p><strong>Upper bound on the bias:</strong> Using the property $\int_{-\infty}^{+\infty}K(u)du = 1$ we get:</p>

<script type="math/tex; mode=display">b_p(x) = \int_{-\infty}^{+\infty}K(u)[p(x + uh) - p(x)]du.</script>

<p>This expression of $b_p(x)$ allows us to see the role of the regularity of $p$. It is natural to think that the most regular $p$ is, the better our estimate will be,  because when making inference about $p(x)$ we only have access to the nearest training example $(X_1,\ldots,X_n)$ which were generated using the amplitude of $p$ near them, so we do not want $p$ to be too wiggly around $x$.</p>

<p>Let’s assume that $p$ is $L$-Lipschitz, where $L$ is a positive real number. This means that $|p(u) - p(v)| \leq L|u-v|$ for all $(u,v) \in \mathbb{R}^2$. Then:</p>

<script type="math/tex; mode=display">|b_p(x)| \leq Lh\int_{-\infty}^{+\infty} K(u)|u|du,</script>

<p>so:</p>

<script type="math/tex; mode=display">b_p(x)^2 \leq C_2h^2,</script>

<p>where $C_2 = L\int_{-\infty}^{+\infty}K(u)|u|du$.</p>

<p>Finally we get that the mean squared error is upper bounded by the sum of two terms :</p>

<script type="math/tex; mode=display">\text{MSE}_p(x) \leq \frac{C_1}{nh} + C_2h^2.</script>

<p><img src="/blog/images/MSE.png" alt="MSE" /></p>

<p>image2</p>

  </div><a class="u-url" href="/blog/2020/02/15/density-estimation.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Samy Clementz</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Samy Clementz</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list">
  <li><a href="https://github.com/SamyClem"><svg class="svg-icon"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg> <span class="username">SamyClem</span></a></li><li><a href="https://www.twitter.com/%40samy_clementz"><svg class="svg-icon"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">@samy_clementz</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p></p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
